{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maintenant on va essayer d'aller plus loin en fine-tunant un modèle pré-entrainé (RoBERTa) sur nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio transformers scikit-learn pandas numpy matplotlib seaborn mlflow skl2onnx onnxruntime requests onnx datasets onnxruntime skl2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elo/Desktop/EICNAM/1er année/ML/projet-ml/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Using device: {device}\")  # Vérifier si MPS est activé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import time\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"bhavyagiri/imdb-spoiler\")\n",
    "data_train = pd.DataFrame(ds['train']).dropna()\n",
    "data_test = pd.DataFrame(ds['validation']).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train['text']\n",
    "y_train = data_train['label']\n",
    "X_test = data_test['text']\n",
    "y_test = data_test['label']\n",
    "\n",
    "# Combiner les features et labels en un seul DataFrame pour faciliter le traitement\n",
    "train_df = pd.DataFrame({'text': X_train, 'label': y_train})\n",
    "test_df = pd.DataFrame({'text': X_test, 'label': y_test})\n",
    "\n",
    "# Séparer les classes\n",
    "train_majority = train_df[train_df['label'] == 0]\n",
    "train_minority = train_df[train_df['label'] == 1]\n",
    "\n",
    "# Sur-échantillonnage de la classe minoritaire\n",
    "train_minority_oversampled = resample(\n",
    "    train_minority,\n",
    "    replace=True,  # Rééchantillonnage avec remise\n",
    "    n_samples=len(train_majority),  # Équilibrer les classes\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fusion des classes équilibrées et mélange\n",
    "train_balanced = pd.concat([train_majority, train_minority_oversampled]).sample(frac=1, random_state=42)\n",
    "\n",
    "# Sélectionner 3000 échantillons pour le training\n",
    "train_balanced = train_balanced.sample(n=3000, random_state=42)\n",
    "\n",
    "# Sélectionner 1000 échantillons pour le test (en conservant la distribution originale)\n",
    "test_sampled = test_df.sample(n=1000, random_state=42)\n",
    "\n",
    "# Mise à jour des variables finales\n",
    "X_train = train_balanced['text']\n",
    "y_train = train_balanced['label']\n",
    "X_test = test_sampled['text']\n",
    "y_test = test_sampled['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(texts, tokenizer, max_length=512):\n",
    "    return tokenizer(\n",
    "        texts, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=max_length, \n",
    "        return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = tokenizer(list(X_train), padding = True, truncation=True)\n",
    "test_tokens = tokenizer(list(X_test), padding = True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 16357, 3854, 25487, 2003, 2028, 1997, 2216, 2396, 2160, 5501, 1045, 2074, 2064, 1005, 1056, 5993, 2007, 2043, 2009, 3310, 2000, 2010, 7696, 2005, 1996, 2087, 2112, 1010, 2174, 1045, 2064, 2036, 4066, 1997, 3305, 2339, 2002, 6732, 1996, 2126, 2002, 2515, 1012, 2025, 2000, 2272, 2125, 2969, 1011, 27427, 5313, 11461, 1010, 2021, 2002, 2003, 5399, 3305, 3085, 2000, 1037, 2391, 1012, 2002, 3849, 2000, 2022, 1037, 3124, 2007, 1037, 2843, 1997, 4301, 1998, 4784, 2055, 8438, 2130, 2065, 2009, 2788, 5260, 2000, 6245, 1998, 10089, 1006, 2029, 2024, 2941, 2261, 3114, 2339, 2111, 2175, 2000, 3422, 5691, 1007, 1012, 3071, 2038, 2045, 10740, 1998, 2070, 2089, 2272, 2125, 2200, 2367, 2013, 1996, 2591, 13373, 1010, 1998, 2023, 3185, 2003, 2200, 2367, 2013, 1996, 2591, 13373, 1012, 2023, 8857, 3262, 2006, 2108, 1037, 6396, 8737, 23393, 13241, 2278, 1998, 1996, 14337, 2791, 1997, 14938, 1012, 2009, 1005, 1055, 2066, 16357, 4117, 3770, 1003, 2010, 4301, 2055, 3348, 1998, 2322, 1003, 2055, 1996, 2060, 4933, 1010, 2066, 8317, 9273, 3388, 1998, 20625, 2791, 1012, 16357, 2003, 1037, 2472, 2008, 7459, 11463, 2319, 9905, 6632, 1999, 2396, 1010, 2947, 8509, 2008, 5783, 1999, 2010, 5691, 2066, 2023, 2028, 1012, 1996, 2364, 2466, 22901, 2007, 1996, 8290, 2090, 1037, 2931, 6396, 8737, 23393, 13241, 2278, 3533, 1006, 5904, 12154, 20431, 1007, 1998, 1996, 8317, 2135, 10250, 19879, 6024, 2711, 7367, 3669, 25494, 1006, 11894, 2078, 24053, 2869, 13444, 1007, 1012, 1998, 2009, 3972, 6961, 2046, 1996, 14967, 2046, 2129, 1996, 6396, 8737, 6806, 2234, 2000, 2022, 1999, 2014, 3663, 2083, 2014, 2627, 2466, 2013, 2108, 1037, 4845, 2000, 1037, 9458, 1012, 2007, 2014, 2034, 16985, 28357, 6298, 4432, 2043, 2009, 2234, 2000, 3348, 1010, 2021, 2145, 2481, 1005, 1056, 2128, 4115, 15549, 4095, 2014, 21810, 2005, 5782, 2062, 1012, 2009, 1005, 1055, 10468, 1037, 2466, 1997, 3533, 1005, 1055, 10138, 1998, 4990, 2083, 2014, 4424, 28809, 1012, 2054, 1045, 2481, 1005, 1056, 3305, 2001, 2339, 3533, 2052, 2031, 3348, 2007, 6721, 2111, 2074, 2005, 1037, 4524, 1997, 9485, 1012, 2009, 2174, 3972, 6961, 2046, 1996, 27952, 1997, 3533, 1010, 2070, 1045, 2071, 17704, 3305, 1998, 2060, 3033, 1045, 2074, 2481, 1005, 1056, 2138, 2009, 2074, 2134, 1005, 1056, 2191, 2151, 3168, 2000, 2033, 1012, 1045, 7714, 2179, 3056, 13425, 1998, 10266, 2043, 2009, 3310, 2000, 1996, 2529, 2568, 2000, 2022, 5875, 1012, 20474, 6845, 8780, 16093, 2003, 2036, 2204, 1999, 2023, 1010, 2021, 2008, 2003, 2138, 2002, 2003, 2200, 2204, 2012, 2652, 15703, 15536, 8737, 2100, 2079, 19140, 3494, 1012, 2469, 2023, 3185, 2515, 2031, 2070, 3653, 6528, 20771, 2969, 1011, 27427, 5313, 11461, 3787, 1010, 2021, 2087, 2396, 2160, 5691, 2024, 2006, 1996, 3653, 6528, 20771, 1998, 2969, 1011, 27427, 5313, 11461, 2217, 1997, 2477, 1012, 1998, 2023, 3185, 2003, 21834, 2094, 2007, 3746, 5134, 2008, 14396, 2000, 3348, 2043, 2009, 3310, 1996, 2529, 2568, 2029, 2070, 9501, 2453, 2424, 23824, 1012, 2153, 2070, 2089, 5993, 2007, 1010, 2500, 2180, 1005, 1056, 2043, 2009, 3310, 2000, 1996, 7696, 1997, 2023, 3185, 1012, 2070, 2089, 2360, 2023, 2003, 19378, 1998, 1045, 17704, 102]\n",
      "[CLS] lars von trier is one of those art house directors i just can't agree with when it comes to his messages for the most part, however i can also sort of understand why he thinks the way he does. not to come off self - indulgent, but he is somewhat understandable to a point. he seems to be a guy with a lot of thoughts and ideas about humanity even if it usually leads to depression and anxiety ( which are actually few reason why people go to watch movies ). everyone has there opinions and some may come off very different from the social norm, and this movie is very different from the social norm. this centered mostly on being a nymphomaniac and the selfishness of mankind. it's like lars combined 80 % his thoughts about sex and 20 % about the other stuff, like psychological mindset and hopelessness. lars is a director that loves melancholia in art, thus puts that element in his movies like this one. the main story progresses with the interaction between a female nymphomaniac joe ( charlotte gainsbourg ) and the psychologically calculative person seligman ( stellan skarsgard ). and it delves into the progression into how the nympho came to be in her situation through her past story from being a kid to a teen. with her first tarnished romantic vision when it came to sex, but still couldn't relinquish her thirst for wanting more. it's basically a story of joe's uprising and journey through her sexual endeavors. what i couldn't understand was why joe would have sex with random people just for a bag of candy. it however delves into the subconscious of joe, some i could kinda understand and other parts i just couldn't because it just didn't make any sense to me. i personally found certain imagery and interactions when it comes to the human mind to be interesting. shia labeouf is also good in this, but that is because he is very good at playing annoying wimpy douche characters. sure this movie does have some pretentious self - indulgent elements, but most art house movies are on the pretentious and self - indulgent side of things. and this movie is riddled with imageries that relate to sex when it comes the human mind which some audiences might find intriguing. again some may agree with, others won't when it comes to the messages of this movie. some may say this is pornography and i kinda [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(train_tokens['input_ids'][0])\n",
    "print(tokenizer.decode(train_tokens['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenData(Dataset):\n",
    "    def __init__(self, train = False):\n",
    "        if train:\n",
    "            self.text_data = X_train\n",
    "            self.tokens = train_tokens\n",
    "            self.labels = list(y_train)\n",
    "        else:\n",
    "            self.text_data = X_test\n",
    "            self.tokens = test_tokens\n",
    "            self.labels = list(y_test)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        for k, v in self.tokens.items():\n",
    "            sample[k] = torch.tensor(v[idx])\n",
    "        sample['labels'] = torch.tensor(self.labels[idx])\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "train_dataset = TokenData(train = True)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TokenData(train = False)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5) # Optimization function\n",
    "loss_fn = torch.nn.CrossEntropyLoss() # Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Époque 1/3\n",
      "[Train] Batch 1/13 - Loss: 0.4811 | Temps écoulé: 98.00s | Temps restant estimé: 3626.08s\n",
      "[Train] Batch 2/13 - Loss: 0.5525 | Temps écoulé: 330.16s | Temps restant estimé: 5942.89s\n",
      "[Train] Batch 3/13 - Loss: 0.5506 | Temps écoulé: 450.10s | Temps restant estimé: 5251.21s\n",
      "[Train] Batch 4/13 - Loss: 0.5424 | Temps écoulé: 571.62s | Temps restant estimé: 4858.80s\n",
      "[Train] Batch 5/13 - Loss: 0.5408 | Temps écoulé: 645.66s | Temps restant estimé: 4261.38s\n",
      "[Train] Batch 6/13 - Loss: 0.5122 | Temps écoulé: 699.48s | Temps restant estimé: 3730.58s\n",
      "[Train] Batch 7/13 - Loss: 0.4910 | Temps écoulé: 753.42s | Temps restant estimé: 3336.58s\n",
      "[Train] Batch 8/13 - Loss: 0.6180 | Temps écoulé: 829.97s | Temps restant estimé: 3112.37s\n",
      "[Train] Batch 9/13 - Loss: 0.4531 | Temps écoulé: 900.94s | Temps restant estimé: 2903.01s\n",
      "[Train] Batch 10/13 - Loss: 0.5713 | Temps écoulé: 975.02s | Temps restant estimé: 2730.05s\n",
      "[Train] Batch 11/13 - Loss: 0.6009 | Temps écoulé: 1046.65s | Temps restant estimé: 2569.06s\n",
      "[Train] Batch 12/13 - Loss: 0.4865 | Temps écoulé: 1112.14s | Temps restant estimé: 2409.63s\n",
      "[Train] Batch 13/13 - Loss: 0.4318 | Temps écoulé: 1115.52s | Temps restant estimé: 2145.22s\n",
      "✅ Fin entraînement époque 1 - Loss moyenne: 0.5256 | Temps: 1115.52s\n",
      "[Test] Batch 1/25 - Loss: 0.4759\n",
      "[Test] Batch 2/25 - Loss: 0.5313\n",
      "[Test] Batch 3/25 - Loss: 0.5767\n",
      "[Test] Batch 4/25 - Loss: 0.6050\n",
      "[Test] Batch 5/25 - Loss: 0.5437\n",
      "[Test] Batch 6/25 - Loss: 0.5652\n",
      "[Test] Batch 7/25 - Loss: 0.5840\n",
      "[Test] Batch 8/25 - Loss: 0.6095\n",
      "[Test] Batch 9/25 - Loss: 0.6060\n",
      "[Test] Batch 10/25 - Loss: 0.4948\n",
      "[Test] Batch 11/25 - Loss: 0.5391\n",
      "[Test] Batch 12/25 - Loss: 0.5651\n",
      "[Test] Batch 13/25 - Loss: 0.6011\n",
      "[Test] Batch 14/25 - Loss: 0.6314\n",
      "[Test] Batch 15/25 - Loss: 0.5265\n",
      "[Test] Batch 16/25 - Loss: 0.5836\n",
      "[Test] Batch 17/25 - Loss: 0.5695\n",
      "[Test] Batch 18/25 - Loss: 0.4190\n",
      "[Test] Batch 19/25 - Loss: 0.6273\n",
      "[Test] Batch 20/25 - Loss: 0.4658\n",
      "[Test] Batch 21/25 - Loss: 0.4927\n",
      "[Test] Batch 22/25 - Loss: 0.6254\n",
      "[Test] Batch 23/25 - Loss: 0.5100\n",
      "[Test] Batch 24/25 - Loss: 0.5107\n",
      "[Test] Batch 25/25 - Loss: 0.5957\n",
      "📊 Fin test époque 1 - Loss moyenne: 0.5542 | Accuracy: 71.9000%\n",
      "\n",
      "🔄 Époque 2/3\n",
      "[Train] Batch 1/13 - Loss: 0.5772 | Temps écoulé: 1202.70s | Temps restant estimé: 2061.78s\n",
      "[Train] Batch 2/13 - Loss: 0.4899 | Temps écoulé: 1266.84s | Temps restant estimé: 1942.49s\n",
      "[Train] Batch 3/13 - Loss: 0.4113 | Temps écoulé: 1353.39s | Temps restant estimé: 1860.91s\n",
      "[Train] Batch 4/13 - Loss: 0.5432 | Temps écoulé: 1429.35s | Temps restant estimé: 1765.66s\n",
      "[Train] Batch 5/13 - Loss: 0.5547 | Temps écoulé: 1501.21s | Temps restant estimé: 1668.01s\n",
      "[Train] Batch 6/13 - Loss: 0.5757 | Temps écoulé: 1574.07s | Temps restant estimé: 1574.07s\n",
      "[Train] Batch 7/13 - Loss: 0.4824 | Temps écoulé: 1630.23s | Temps restant estimé: 1467.21s\n",
      "[Train] Batch 8/13 - Loss: 0.5332 | Temps écoulé: 1692.49s | Temps restant estimé: 1370.11s\n",
      "[Train] Batch 9/13 - Loss: 0.4666 | Temps écoulé: 1763.65s | Temps restant estimé: 1282.66s\n",
      "[Train] Batch 10/13 - Loss: 0.5528 | Temps écoulé: 1845.46s | Temps restant estimé: 1203.56s\n",
      "[Train] Batch 11/13 - Loss: 0.5269 | Temps écoulé: 1912.81s | Temps restant estimé: 1115.80s\n",
      "[Train] Batch 12/13 - Loss: 0.5771 | Temps écoulé: 1980.63s | Temps restant estimé: 1029.93s\n",
      "[Train] Batch 13/13 - Loss: 0.3921 | Temps écoulé: 1986.64s | Temps restant estimé: 916.91s\n",
      "✅ Fin entraînement époque 2 - Loss moyenne: 0.5141 | Temps: 851.56s\n",
      "[Test] Batch 1/25 - Loss: 0.5850\n",
      "[Test] Batch 2/25 - Loss: 0.5091\n",
      "[Test] Batch 3/25 - Loss: 0.5729\n",
      "[Test] Batch 4/25 - Loss: 0.5179\n",
      "[Test] Batch 5/25 - Loss: 0.6168\n",
      "[Test] Batch 6/25 - Loss: 0.5339\n",
      "[Test] Batch 7/25 - Loss: 0.3878\n",
      "[Test] Batch 8/25 - Loss: 0.5380\n",
      "[Test] Batch 9/25 - Loss: 0.5996\n",
      "[Test] Batch 10/25 - Loss: 0.6110\n",
      "[Test] Batch 11/25 - Loss: 0.6346\n",
      "[Test] Batch 12/25 - Loss: 0.5895\n",
      "[Test] Batch 13/25 - Loss: 0.4808\n",
      "[Test] Batch 14/25 - Loss: 0.5114\n",
      "[Test] Batch 15/25 - Loss: 0.6770\n",
      "[Test] Batch 16/25 - Loss: 0.5456\n",
      "[Test] Batch 17/25 - Loss: 0.5879\n",
      "[Test] Batch 18/25 - Loss: 0.5169\n",
      "[Test] Batch 19/25 - Loss: 0.4926\n",
      "[Test] Batch 20/25 - Loss: 0.6545\n",
      "[Test] Batch 21/25 - Loss: 0.5148\n",
      "[Test] Batch 22/25 - Loss: 0.5698\n",
      "[Test] Batch 23/25 - Loss: 0.5144\n",
      "[Test] Batch 24/25 - Loss: 0.5306\n",
      "[Test] Batch 25/25 - Loss: 0.4793\n",
      "📊 Fin test époque 2 - Loss moyenne: 0.5509 | Accuracy: 72.1000%\n",
      "\n",
      "🔄 Époque 3/3\n",
      "[Train] Batch 1/13 - Loss: 0.4829 | Temps écoulé: 2081.52s | Temps restant estimé: 848.03s\n",
      "[Train] Batch 2/13 - Loss: 0.5334 | Temps écoulé: 2166.83s | Temps restant estimé: 773.87s\n",
      "[Train] Batch 3/13 - Loss: 0.4736 | Temps écoulé: 2224.31s | Temps restant estimé: 690.30s\n",
      "[Train] Batch 4/13 - Loss: 0.5043 | Temps écoulé: 2300.93s | Temps restant estimé: 613.58s\n",
      "[Train] Batch 5/13 - Loss: 0.5299 | Temps écoulé: 2367.44s | Temps restant estimé: 534.58s\n",
      "[Train] Batch 6/13 - Loss: 0.5166 | Temps écoulé: 2445.73s | Temps restant estimé: 458.58s\n",
      "[Train] Batch 7/13 - Loss: 0.5886 | Temps écoulé: 2514.14s | Temps restant estimé: 380.93s\n",
      "[Train] Batch 8/13 - Loss: 0.4953 | Temps écoulé: 2585.27s | Temps restant estimé: 304.15s\n",
      "[Train] Batch 9/13 - Loss: 0.5109 | Temps écoulé: 2663.02s | Temps restant estimé: 228.26s\n",
      "[Train] Batch 10/13 - Loss: 0.4137 | Temps écoulé: 2725.17s | Temps restant estimé: 151.40s\n",
      "[Train] Batch 11/13 - Loss: 0.4792 | Temps écoulé: 2799.36s | Temps restant estimé: 75.66s\n",
      "[Train] Batch 12/13 - Loss: 0.6446 | Temps écoulé: 2867.12s | Temps restant estimé: 0.00s\n",
      "[Train] Batch 13/13 - Loss: 0.4744 | Temps écoulé: 2869.42s | Temps restant estimé: -73.57s\n",
      "✅ Fin entraînement époque 3 - Loss moyenne: 0.5113 | Temps: 865.22s\n",
      "[Test] Batch 1/25 - Loss: 0.4777\n",
      "[Test] Batch 2/25 - Loss: 0.4224\n",
      "[Test] Batch 3/25 - Loss: 0.6641\n",
      "[Test] Batch 4/25 - Loss: 0.4603\n",
      "[Test] Batch 5/25 - Loss: 0.5846\n",
      "[Test] Batch 6/25 - Loss: 0.5993\n",
      "[Test] Batch 7/25 - Loss: 0.5237\n",
      "[Test] Batch 8/25 - Loss: 0.4879\n",
      "[Test] Batch 9/25 - Loss: 0.5606\n",
      "[Test] Batch 10/25 - Loss: 0.5061\n",
      "[Test] Batch 11/25 - Loss: 0.4555\n",
      "[Test] Batch 12/25 - Loss: 0.5902\n",
      "[Test] Batch 13/25 - Loss: 0.5327\n",
      "[Test] Batch 14/25 - Loss: 0.5486\n",
      "[Test] Batch 15/25 - Loss: 0.5274\n",
      "[Test] Batch 16/25 - Loss: 0.6091\n",
      "[Test] Batch 17/25 - Loss: 0.5717\n",
      "[Test] Batch 18/25 - Loss: 0.5778\n",
      "[Test] Batch 19/25 - Loss: 0.5817\n",
      "[Test] Batch 20/25 - Loss: 0.6102\n",
      "[Test] Batch 21/25 - Loss: 0.4791\n",
      "[Test] Batch 22/25 - Loss: 0.6231\n",
      "[Test] Batch 23/25 - Loss: 0.6217\n",
      "[Test] Batch 24/25 - Loss: 0.5135\n",
      "[Test] Batch 25/25 - Loss: 0.5362\n",
      "📊 Fin test époque 3 - Loss moyenne: 0.5466 | Accuracy: 72.7000%\n",
      "\n",
      "⏳ Entraînement terminé en 2887.10s\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "total_batches = len(train_loader) + len(test_loader)\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n🔄 Époque {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    # Phase d'entraînement\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    start_epoch_time = time.time()\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        loss = loss_fn(outputs.logits, batch['labels'])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_time = (elapsed_time / ((epoch * len(train_loader)) + i + 1)) * (total_batches - ((epoch * len(train_loader)) + i + 1))\n",
    "\n",
    "        print(f\"[Train] Batch {i+1}/{len(train_loader)} - Loss: {loss.item():.4f} | Temps écoulé: {elapsed_time:.2f}s | Temps restant estimé: {remaining_time:.2f}s\")\n",
    "\n",
    "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "    print(f\"✅ Fin entraînement époque {epoch+1} - Loss moyenne: {avg_train_loss:.4f} | Temps: {time.time() - start_epoch_time:.2f}s\")\n",
    "\n",
    "    # Phase de test\n",
    "    model.eval()\n",
    "    epoch_test_loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "\n",
    "            loss = loss_fn(outputs.logits, batch['labels'])\n",
    "            epoch_test_loss += loss.item()\n",
    "\n",
    "            preds = outputs.logits.argmax(dim=1)\n",
    "            correct += (preds == batch['labels']).sum().item()\n",
    "            total_samples += batch['labels'].size(0)\n",
    "\n",
    "            print(f\"[Test] Batch {i+1}/{len(test_loader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_test_loss = epoch_test_loss / len(test_loader)\n",
    "    accuracy = correct / total_samples\n",
    "\n",
    "    print(f\"📊 Fin test époque {epoch+1} - Loss moyenne: {avg_test_loss:.4f} | Accuracy: {accuracy:.4%}\")\n",
    "\n",
    "print(f\"\\n⏳ Entraînement terminé en {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss_fn': loss_fn,\n",
    "}, \"checkpoint_final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test du modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"models/checkpoint_final.pth\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "loss_fn = checkpoint['loss_fn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réduit le learning rate pour éviter de trop déformer le modèle pré-entrainé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts(texts, tokenizer, max_length=512):\n",
    "    encodings = tokenizer(\n",
    "        texts, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=max_length, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return {key: tensor.to(device) for key, tensor in encodings.items()} \n",
    "\n",
    "def make_inference(texts):\n",
    "    inputs = preprocess_texts(texts, tokenizer)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "    return predictions.cpu().numpy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    \"I love this movie but at the end we can see Aurore see her father that's strange\",\n",
    "    \"The plot twist was completely unexpected and brilliant!\",\n",
    "    \"Bob is the real villain of the story, and he dies in the end.\",\n",
    "]\n",
    "\n",
    "type(make_inference(texts)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
