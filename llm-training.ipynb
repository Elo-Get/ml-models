{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maintenant on va essayer d'aller plus loin en fine-tunant un mod√®le pr√©-entrain√© (RoBERTa) sur nos donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio transformers scikit-learn pandas numpy matplotlib seaborn mlflow skl2onnx onnxruntime requests onnx datasets onnxruntime skl2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elo/Desktop/EICNAM/1er anneÃÅe/ML/projet-ml/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Using device: {device}\")  # V√©rifier si MPS est activ√©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import time\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"bhavyagiri/imdb-spoiler\")\n",
    "data_train = pd.DataFrame(ds['train']).dropna()\n",
    "data_test = pd.DataFrame(ds['validation']).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train['text']\n",
    "y_train = data_train['label']\n",
    "X_test = data_test['text']\n",
    "y_test = data_test['label']\n",
    "\n",
    "# Combiner les features et labels en un seul DataFrame pour faciliter le traitement\n",
    "train_df = pd.DataFrame({'text': X_train, 'label': y_train})\n",
    "test_df = pd.DataFrame({'text': X_test, 'label': y_test})\n",
    "\n",
    "# S√©parer les classes\n",
    "train_majority = train_df[train_df['label'] == 0]\n",
    "train_minority = train_df[train_df['label'] == 1]\n",
    "\n",
    "# Sur-√©chantillonnage de la classe minoritaire\n",
    "train_minority_oversampled = resample(\n",
    "    train_minority,\n",
    "    replace=True,  # R√©√©chantillonnage avec remise\n",
    "    n_samples=len(train_majority),  # √âquilibrer les classes\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fusion des classes √©quilibr√©es et m√©lange\n",
    "train_balanced = pd.concat([train_majority, train_minority_oversampled]).sample(frac=1, random_state=42)\n",
    "\n",
    "# S√©lectionner 3000 √©chantillons pour le training\n",
    "train_balanced = train_balanced.sample(n=3000, random_state=42)\n",
    "\n",
    "# S√©lectionner 1000 √©chantillons pour le test (en conservant la distribution originale)\n",
    "test_sampled = test_df.sample(n=1000, random_state=42)\n",
    "\n",
    "# Mise √† jour des variables finales\n",
    "X_train = train_balanced['text']\n",
    "y_train = train_balanced['label']\n",
    "X_test = test_sampled['text']\n",
    "y_test = test_sampled['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(texts, tokenizer, max_length=512):\n",
    "    return tokenizer(\n",
    "        texts, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=max_length, \n",
    "        return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = tokenizer(list(X_train), padding = True, truncation=True)\n",
    "test_tokens = tokenizer(list(X_test), padding = True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 16357, 3854, 25487, 2003, 2028, 1997, 2216, 2396, 2160, 5501, 1045, 2074, 2064, 1005, 1056, 5993, 2007, 2043, 2009, 3310, 2000, 2010, 7696, 2005, 1996, 2087, 2112, 1010, 2174, 1045, 2064, 2036, 4066, 1997, 3305, 2339, 2002, 6732, 1996, 2126, 2002, 2515, 1012, 2025, 2000, 2272, 2125, 2969, 1011, 27427, 5313, 11461, 1010, 2021, 2002, 2003, 5399, 3305, 3085, 2000, 1037, 2391, 1012, 2002, 3849, 2000, 2022, 1037, 3124, 2007, 1037, 2843, 1997, 4301, 1998, 4784, 2055, 8438, 2130, 2065, 2009, 2788, 5260, 2000, 6245, 1998, 10089, 1006, 2029, 2024, 2941, 2261, 3114, 2339, 2111, 2175, 2000, 3422, 5691, 1007, 1012, 3071, 2038, 2045, 10740, 1998, 2070, 2089, 2272, 2125, 2200, 2367, 2013, 1996, 2591, 13373, 1010, 1998, 2023, 3185, 2003, 2200, 2367, 2013, 1996, 2591, 13373, 1012, 2023, 8857, 3262, 2006, 2108, 1037, 6396, 8737, 23393, 13241, 2278, 1998, 1996, 14337, 2791, 1997, 14938, 1012, 2009, 1005, 1055, 2066, 16357, 4117, 3770, 1003, 2010, 4301, 2055, 3348, 1998, 2322, 1003, 2055, 1996, 2060, 4933, 1010, 2066, 8317, 9273, 3388, 1998, 20625, 2791, 1012, 16357, 2003, 1037, 2472, 2008, 7459, 11463, 2319, 9905, 6632, 1999, 2396, 1010, 2947, 8509, 2008, 5783, 1999, 2010, 5691, 2066, 2023, 2028, 1012, 1996, 2364, 2466, 22901, 2007, 1996, 8290, 2090, 1037, 2931, 6396, 8737, 23393, 13241, 2278, 3533, 1006, 5904, 12154, 20431, 1007, 1998, 1996, 8317, 2135, 10250, 19879, 6024, 2711, 7367, 3669, 25494, 1006, 11894, 2078, 24053, 2869, 13444, 1007, 1012, 1998, 2009, 3972, 6961, 2046, 1996, 14967, 2046, 2129, 1996, 6396, 8737, 6806, 2234, 2000, 2022, 1999, 2014, 3663, 2083, 2014, 2627, 2466, 2013, 2108, 1037, 4845, 2000, 1037, 9458, 1012, 2007, 2014, 2034, 16985, 28357, 6298, 4432, 2043, 2009, 2234, 2000, 3348, 1010, 2021, 2145, 2481, 1005, 1056, 2128, 4115, 15549, 4095, 2014, 21810, 2005, 5782, 2062, 1012, 2009, 1005, 1055, 10468, 1037, 2466, 1997, 3533, 1005, 1055, 10138, 1998, 4990, 2083, 2014, 4424, 28809, 1012, 2054, 1045, 2481, 1005, 1056, 3305, 2001, 2339, 3533, 2052, 2031, 3348, 2007, 6721, 2111, 2074, 2005, 1037, 4524, 1997, 9485, 1012, 2009, 2174, 3972, 6961, 2046, 1996, 27952, 1997, 3533, 1010, 2070, 1045, 2071, 17704, 3305, 1998, 2060, 3033, 1045, 2074, 2481, 1005, 1056, 2138, 2009, 2074, 2134, 1005, 1056, 2191, 2151, 3168, 2000, 2033, 1012, 1045, 7714, 2179, 3056, 13425, 1998, 10266, 2043, 2009, 3310, 2000, 1996, 2529, 2568, 2000, 2022, 5875, 1012, 20474, 6845, 8780, 16093, 2003, 2036, 2204, 1999, 2023, 1010, 2021, 2008, 2003, 2138, 2002, 2003, 2200, 2204, 2012, 2652, 15703, 15536, 8737, 2100, 2079, 19140, 3494, 1012, 2469, 2023, 3185, 2515, 2031, 2070, 3653, 6528, 20771, 2969, 1011, 27427, 5313, 11461, 3787, 1010, 2021, 2087, 2396, 2160, 5691, 2024, 2006, 1996, 3653, 6528, 20771, 1998, 2969, 1011, 27427, 5313, 11461, 2217, 1997, 2477, 1012, 1998, 2023, 3185, 2003, 21834, 2094, 2007, 3746, 5134, 2008, 14396, 2000, 3348, 2043, 2009, 3310, 1996, 2529, 2568, 2029, 2070, 9501, 2453, 2424, 23824, 1012, 2153, 2070, 2089, 5993, 2007, 1010, 2500, 2180, 1005, 1056, 2043, 2009, 3310, 2000, 1996, 7696, 1997, 2023, 3185, 1012, 2070, 2089, 2360, 2023, 2003, 19378, 1998, 1045, 17704, 102]\n",
      "[CLS] lars von trier is one of those art house directors i just can't agree with when it comes to his messages for the most part, however i can also sort of understand why he thinks the way he does. not to come off self - indulgent, but he is somewhat understandable to a point. he seems to be a guy with a lot of thoughts and ideas about humanity even if it usually leads to depression and anxiety ( which are actually few reason why people go to watch movies ). everyone has there opinions and some may come off very different from the social norm, and this movie is very different from the social norm. this centered mostly on being a nymphomaniac and the selfishness of mankind. it's like lars combined 80 % his thoughts about sex and 20 % about the other stuff, like psychological mindset and hopelessness. lars is a director that loves melancholia in art, thus puts that element in his movies like this one. the main story progresses with the interaction between a female nymphomaniac joe ( charlotte gainsbourg ) and the psychologically calculative person seligman ( stellan skarsgard ). and it delves into the progression into how the nympho came to be in her situation through her past story from being a kid to a teen. with her first tarnished romantic vision when it came to sex, but still couldn't relinquish her thirst for wanting more. it's basically a story of joe's uprising and journey through her sexual endeavors. what i couldn't understand was why joe would have sex with random people just for a bag of candy. it however delves into the subconscious of joe, some i could kinda understand and other parts i just couldn't because it just didn't make any sense to me. i personally found certain imagery and interactions when it comes to the human mind to be interesting. shia labeouf is also good in this, but that is because he is very good at playing annoying wimpy douche characters. sure this movie does have some pretentious self - indulgent elements, but most art house movies are on the pretentious and self - indulgent side of things. and this movie is riddled with imageries that relate to sex when it comes the human mind which some audiences might find intriguing. again some may agree with, others won't when it comes to the messages of this movie. some may say this is pornography and i kinda [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(train_tokens['input_ids'][0])\n",
    "print(tokenizer.decode(train_tokens['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenData(Dataset):\n",
    "    def __init__(self, train = False):\n",
    "        if train:\n",
    "            self.text_data = X_train\n",
    "            self.tokens = train_tokens\n",
    "            self.labels = list(y_train)\n",
    "        else:\n",
    "            self.text_data = X_test\n",
    "            self.tokens = test_tokens\n",
    "            self.labels = list(y_test)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        for k, v in self.tokens.items():\n",
    "            sample[k] = torch.tensor(v[idx])\n",
    "        sample['labels'] = torch.tensor(self.labels[idx])\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "train_dataset = TokenData(train = True)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TokenData(train = False)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5) # Optimization function\n",
    "loss_fn = torch.nn.CrossEntropyLoss() # Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ √âpoque 1/3\n",
      "[Train] Batch 1/13 - Loss: 0.4811 | Temps √©coul√©: 98.00s | Temps restant estim√©: 3626.08s\n",
      "[Train] Batch 2/13 - Loss: 0.5525 | Temps √©coul√©: 330.16s | Temps restant estim√©: 5942.89s\n",
      "[Train] Batch 3/13 - Loss: 0.5506 | Temps √©coul√©: 450.10s | Temps restant estim√©: 5251.21s\n",
      "[Train] Batch 4/13 - Loss: 0.5424 | Temps √©coul√©: 571.62s | Temps restant estim√©: 4858.80s\n",
      "[Train] Batch 5/13 - Loss: 0.5408 | Temps √©coul√©: 645.66s | Temps restant estim√©: 4261.38s\n",
      "[Train] Batch 6/13 - Loss: 0.5122 | Temps √©coul√©: 699.48s | Temps restant estim√©: 3730.58s\n",
      "[Train] Batch 7/13 - Loss: 0.4910 | Temps √©coul√©: 753.42s | Temps restant estim√©: 3336.58s\n",
      "[Train] Batch 8/13 - Loss: 0.6180 | Temps √©coul√©: 829.97s | Temps restant estim√©: 3112.37s\n",
      "[Train] Batch 9/13 - Loss: 0.4531 | Temps √©coul√©: 900.94s | Temps restant estim√©: 2903.01s\n",
      "[Train] Batch 10/13 - Loss: 0.5713 | Temps √©coul√©: 975.02s | Temps restant estim√©: 2730.05s\n",
      "[Train] Batch 11/13 - Loss: 0.6009 | Temps √©coul√©: 1046.65s | Temps restant estim√©: 2569.06s\n",
      "[Train] Batch 12/13 - Loss: 0.4865 | Temps √©coul√©: 1112.14s | Temps restant estim√©: 2409.63s\n",
      "[Train] Batch 13/13 - Loss: 0.4318 | Temps √©coul√©: 1115.52s | Temps restant estim√©: 2145.22s\n",
      "‚úÖ Fin entra√Ænement √©poque 1 - Loss moyenne: 0.5256 | Temps: 1115.52s\n",
      "[Test] Batch 1/25 - Loss: 0.4759\n",
      "[Test] Batch 2/25 - Loss: 0.5313\n",
      "[Test] Batch 3/25 - Loss: 0.5767\n",
      "[Test] Batch 4/25 - Loss: 0.6050\n",
      "[Test] Batch 5/25 - Loss: 0.5437\n",
      "[Test] Batch 6/25 - Loss: 0.5652\n",
      "[Test] Batch 7/25 - Loss: 0.5840\n",
      "[Test] Batch 8/25 - Loss: 0.6095\n",
      "[Test] Batch 9/25 - Loss: 0.6060\n",
      "[Test] Batch 10/25 - Loss: 0.4948\n",
      "[Test] Batch 11/25 - Loss: 0.5391\n",
      "[Test] Batch 12/25 - Loss: 0.5651\n",
      "[Test] Batch 13/25 - Loss: 0.6011\n",
      "[Test] Batch 14/25 - Loss: 0.6314\n",
      "[Test] Batch 15/25 - Loss: 0.5265\n",
      "[Test] Batch 16/25 - Loss: 0.5836\n",
      "[Test] Batch 17/25 - Loss: 0.5695\n",
      "[Test] Batch 18/25 - Loss: 0.4190\n",
      "[Test] Batch 19/25 - Loss: 0.6273\n",
      "[Test] Batch 20/25 - Loss: 0.4658\n",
      "[Test] Batch 21/25 - Loss: 0.4927\n",
      "[Test] Batch 22/25 - Loss: 0.6254\n",
      "[Test] Batch 23/25 - Loss: 0.5100\n",
      "[Test] Batch 24/25 - Loss: 0.5107\n",
      "[Test] Batch 25/25 - Loss: 0.5957\n",
      "üìä Fin test √©poque 1 - Loss moyenne: 0.5542 | Accuracy: 71.9000%\n",
      "\n",
      "üîÑ √âpoque 2/3\n",
      "[Train] Batch 1/13 - Loss: 0.5772 | Temps √©coul√©: 1202.70s | Temps restant estim√©: 2061.78s\n",
      "[Train] Batch 2/13 - Loss: 0.4899 | Temps √©coul√©: 1266.84s | Temps restant estim√©: 1942.49s\n",
      "[Train] Batch 3/13 - Loss: 0.4113 | Temps √©coul√©: 1353.39s | Temps restant estim√©: 1860.91s\n",
      "[Train] Batch 4/13 - Loss: 0.5432 | Temps √©coul√©: 1429.35s | Temps restant estim√©: 1765.66s\n",
      "[Train] Batch 5/13 - Loss: 0.5547 | Temps √©coul√©: 1501.21s | Temps restant estim√©: 1668.01s\n",
      "[Train] Batch 6/13 - Loss: 0.5757 | Temps √©coul√©: 1574.07s | Temps restant estim√©: 1574.07s\n",
      "[Train] Batch 7/13 - Loss: 0.4824 | Temps √©coul√©: 1630.23s | Temps restant estim√©: 1467.21s\n",
      "[Train] Batch 8/13 - Loss: 0.5332 | Temps √©coul√©: 1692.49s | Temps restant estim√©: 1370.11s\n",
      "[Train] Batch 9/13 - Loss: 0.4666 | Temps √©coul√©: 1763.65s | Temps restant estim√©: 1282.66s\n",
      "[Train] Batch 10/13 - Loss: 0.5528 | Temps √©coul√©: 1845.46s | Temps restant estim√©: 1203.56s\n",
      "[Train] Batch 11/13 - Loss: 0.5269 | Temps √©coul√©: 1912.81s | Temps restant estim√©: 1115.80s\n",
      "[Train] Batch 12/13 - Loss: 0.5771 | Temps √©coul√©: 1980.63s | Temps restant estim√©: 1029.93s\n",
      "[Train] Batch 13/13 - Loss: 0.3921 | Temps √©coul√©: 1986.64s | Temps restant estim√©: 916.91s\n",
      "‚úÖ Fin entra√Ænement √©poque 2 - Loss moyenne: 0.5141 | Temps: 851.56s\n",
      "[Test] Batch 1/25 - Loss: 0.5850\n",
      "[Test] Batch 2/25 - Loss: 0.5091\n",
      "[Test] Batch 3/25 - Loss: 0.5729\n",
      "[Test] Batch 4/25 - Loss: 0.5179\n",
      "[Test] Batch 5/25 - Loss: 0.6168\n",
      "[Test] Batch 6/25 - Loss: 0.5339\n",
      "[Test] Batch 7/25 - Loss: 0.3878\n",
      "[Test] Batch 8/25 - Loss: 0.5380\n",
      "[Test] Batch 9/25 - Loss: 0.5996\n",
      "[Test] Batch 10/25 - Loss: 0.6110\n",
      "[Test] Batch 11/25 - Loss: 0.6346\n",
      "[Test] Batch 12/25 - Loss: 0.5895\n",
      "[Test] Batch 13/25 - Loss: 0.4808\n",
      "[Test] Batch 14/25 - Loss: 0.5114\n",
      "[Test] Batch 15/25 - Loss: 0.6770\n",
      "[Test] Batch 16/25 - Loss: 0.5456\n",
      "[Test] Batch 17/25 - Loss: 0.5879\n",
      "[Test] Batch 18/25 - Loss: 0.5169\n",
      "[Test] Batch 19/25 - Loss: 0.4926\n",
      "[Test] Batch 20/25 - Loss: 0.6545\n",
      "[Test] Batch 21/25 - Loss: 0.5148\n",
      "[Test] Batch 22/25 - Loss: 0.5698\n",
      "[Test] Batch 23/25 - Loss: 0.5144\n",
      "[Test] Batch 24/25 - Loss: 0.5306\n",
      "[Test] Batch 25/25 - Loss: 0.4793\n",
      "üìä Fin test √©poque 2 - Loss moyenne: 0.5509 | Accuracy: 72.1000%\n",
      "\n",
      "üîÑ √âpoque 3/3\n",
      "[Train] Batch 1/13 - Loss: 0.4829 | Temps √©coul√©: 2081.52s | Temps restant estim√©: 848.03s\n",
      "[Train] Batch 2/13 - Loss: 0.5334 | Temps √©coul√©: 2166.83s | Temps restant estim√©: 773.87s\n",
      "[Train] Batch 3/13 - Loss: 0.4736 | Temps √©coul√©: 2224.31s | Temps restant estim√©: 690.30s\n",
      "[Train] Batch 4/13 - Loss: 0.5043 | Temps √©coul√©: 2300.93s | Temps restant estim√©: 613.58s\n",
      "[Train] Batch 5/13 - Loss: 0.5299 | Temps √©coul√©: 2367.44s | Temps restant estim√©: 534.58s\n",
      "[Train] Batch 6/13 - Loss: 0.5166 | Temps √©coul√©: 2445.73s | Temps restant estim√©: 458.58s\n",
      "[Train] Batch 7/13 - Loss: 0.5886 | Temps √©coul√©: 2514.14s | Temps restant estim√©: 380.93s\n",
      "[Train] Batch 8/13 - Loss: 0.4953 | Temps √©coul√©: 2585.27s | Temps restant estim√©: 304.15s\n",
      "[Train] Batch 9/13 - Loss: 0.5109 | Temps √©coul√©: 2663.02s | Temps restant estim√©: 228.26s\n",
      "[Train] Batch 10/13 - Loss: 0.4137 | Temps √©coul√©: 2725.17s | Temps restant estim√©: 151.40s\n",
      "[Train] Batch 11/13 - Loss: 0.4792 | Temps √©coul√©: 2799.36s | Temps restant estim√©: 75.66s\n",
      "[Train] Batch 12/13 - Loss: 0.6446 | Temps √©coul√©: 2867.12s | Temps restant estim√©: 0.00s\n",
      "[Train] Batch 13/13 - Loss: 0.4744 | Temps √©coul√©: 2869.42s | Temps restant estim√©: -73.57s\n",
      "‚úÖ Fin entra√Ænement √©poque 3 - Loss moyenne: 0.5113 | Temps: 865.22s\n",
      "[Test] Batch 1/25 - Loss: 0.4777\n",
      "[Test] Batch 2/25 - Loss: 0.4224\n",
      "[Test] Batch 3/25 - Loss: 0.6641\n",
      "[Test] Batch 4/25 - Loss: 0.4603\n",
      "[Test] Batch 5/25 - Loss: 0.5846\n",
      "[Test] Batch 6/25 - Loss: 0.5993\n",
      "[Test] Batch 7/25 - Loss: 0.5237\n",
      "[Test] Batch 8/25 - Loss: 0.4879\n",
      "[Test] Batch 9/25 - Loss: 0.5606\n",
      "[Test] Batch 10/25 - Loss: 0.5061\n",
      "[Test] Batch 11/25 - Loss: 0.4555\n",
      "[Test] Batch 12/25 - Loss: 0.5902\n",
      "[Test] Batch 13/25 - Loss: 0.5327\n",
      "[Test] Batch 14/25 - Loss: 0.5486\n",
      "[Test] Batch 15/25 - Loss: 0.5274\n",
      "[Test] Batch 16/25 - Loss: 0.6091\n",
      "[Test] Batch 17/25 - Loss: 0.5717\n",
      "[Test] Batch 18/25 - Loss: 0.5778\n",
      "[Test] Batch 19/25 - Loss: 0.5817\n",
      "[Test] Batch 20/25 - Loss: 0.6102\n",
      "[Test] Batch 21/25 - Loss: 0.4791\n",
      "[Test] Batch 22/25 - Loss: 0.6231\n",
      "[Test] Batch 23/25 - Loss: 0.6217\n",
      "[Test] Batch 24/25 - Loss: 0.5135\n",
      "[Test] Batch 25/25 - Loss: 0.5362\n",
      "üìä Fin test √©poque 3 - Loss moyenne: 0.5466 | Accuracy: 72.7000%\n",
      "\n",
      "‚è≥ Entra√Ænement termin√© en 2887.10s\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "total_batches = len(train_loader) + len(test_loader)\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nüîÑ √âpoque {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    # Phase d'entra√Ænement\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    start_epoch_time = time.time()\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        loss = loss_fn(outputs.logits, batch['labels'])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_time = (elapsed_time / ((epoch * len(train_loader)) + i + 1)) * (total_batches - ((epoch * len(train_loader)) + i + 1))\n",
    "\n",
    "        print(f\"[Train] Batch {i+1}/{len(train_loader)} - Loss: {loss.item():.4f} | Temps √©coul√©: {elapsed_time:.2f}s | Temps restant estim√©: {remaining_time:.2f}s\")\n",
    "\n",
    "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "    print(f\"‚úÖ Fin entra√Ænement √©poque {epoch+1} - Loss moyenne: {avg_train_loss:.4f} | Temps: {time.time() - start_epoch_time:.2f}s\")\n",
    "\n",
    "    # Phase de test\n",
    "    model.eval()\n",
    "    epoch_test_loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "\n",
    "            loss = loss_fn(outputs.logits, batch['labels'])\n",
    "            epoch_test_loss += loss.item()\n",
    "\n",
    "            preds = outputs.logits.argmax(dim=1)\n",
    "            correct += (preds == batch['labels']).sum().item()\n",
    "            total_samples += batch['labels'].size(0)\n",
    "\n",
    "            print(f\"[Test] Batch {i+1}/{len(test_loader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_test_loss = epoch_test_loss / len(test_loader)\n",
    "    accuracy = correct / total_samples\n",
    "\n",
    "    print(f\"üìä Fin test √©poque {epoch+1} - Loss moyenne: {avg_test_loss:.4f} | Accuracy: {accuracy:.4%}\")\n",
    "\n",
    "print(f\"\\n‚è≥ Entra√Ænement termin√© en {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss_fn': loss_fn,\n",
    "}, \"checkpoint_final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test du modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"models/checkpoint_final.pth\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "loss_fn = checkpoint['loss_fn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On r√©duit le learning rate pour √©viter de trop d√©former le mod√®le pr√©-entrain√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts(texts, tokenizer, max_length=512):\n",
    "    encodings = tokenizer(\n",
    "        texts, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=max_length, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return {key: tensor.to(device) for key, tensor in encodings.items()} \n",
    "\n",
    "def make_inference(texts):\n",
    "    inputs = preprocess_texts(texts, tokenizer)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "    return predictions.cpu().numpy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    \"I love this movie but at the end we can see Aurore see her father that's strange\",\n",
    "    \"The plot twist was completely unexpected and brilliant!\",\n",
    "    \"Bob is the real villain of the story, and he dies in the end.\",\n",
    "]\n",
    "\n",
    "type(make_inference(texts)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
